{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "\n",
    "In order to use the previous methods we need a discrete environment(virtually non-existent in real world) and we need to know the probabilities of state transitions. \n",
    "\n",
    "1. Split up cartpole environment into bins (upper/lower bound of scalar values makes 1 bin)\n",
    "2. learn the transition with experience replay to estimate them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Frozen Lake with Value Iteration\n",
    "\n",
    "we need 3 dictionaries\n",
    "\n",
    "Rewards: key is [s,a,s'], value is the immediate reward \n",
    "<br>\n",
    "Transitions: key is [s,a], value is a dict of s' and the frequency it occured \n",
    "<br>\n",
    "Values: key is [s], value is the calculated value of being in that state\n",
    "\n",
    "\n",
    "Steps:\n",
    "1. Initialise: all values in our value table to zero\n",
    "2. Explore: 100 random steps, keep track of transitions and rewards\n",
    "3. Update: For every state s and every action a in this state, calculate the state action values. In the values table, set the value of state s to the max state action value.\n",
    "4. Test: Run several test episodes to check the average reward of our agent and if it has solved the environment\n",
    "5. Repeat: Go back to step 2 and repeat the process until the environment has been solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import collections\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20\n",
    "EXPLORATION_STEPS = 100\n",
    "SOLVED = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "values = collections.defaultdict(float)\n",
    "transitions = collections.defaultdict(collections.Counter)\n",
    "rewards = collections.defaultdict(float)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "\n",
    "epoch = 0\n",
    "threshold = 0\n",
    "reward = 0\n",
    "\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_q_value(state, action):\n",
    "    t_count = transitions[(state,action)]\n",
    "    t_sum = sum(t_count.values())\n",
    "    value = 0.0\n",
    "    \n",
    "    for target_state, count in t_count.items():\n",
    "        r = rewards[(state,action, target_state)]\n",
    "        trans_p = (count/t_sum)\n",
    "        value += trans_p * (r + GAMMA * values[target_state])\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore(steps):\n",
    "    state = env.reset()\n",
    "    for i in range(steps):\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, is_done, _ = env.step(action)\n",
    "        transitions[(state, action)][new_state] += 1\n",
    "        rewards[(state, action, new_state)] = reward\n",
    "        state = env.reset() if is_done else new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update():\n",
    "    for state in range(n_states):\n",
    "        state_values = [get_q_value(state, action) for action in range(n_actions)]\n",
    "        #Update the values table with the max of Q value of s,a\n",
    "        values[state] = max(state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(episodes):\n",
    "    reward = 0\n",
    "    for e in range(episodes):\n",
    "        reward += play_episode(env)\n",
    "        \n",
    "    return reward/episodes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    best_action, best_value = None, None\n",
    "    for action in range(env.action_space.n):\n",
    "        action_value = get_q_value(state, action)\n",
    "        if best_value is None or best_value < action_value:\n",
    "            best_value = action_value\n",
    "            best_action = action\n",
    "\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_episode( env):\n",
    "    total_reward = 0.0\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = select_action(state)\n",
    "        new_state, reward, is_done, _ = env.step(action)\n",
    "        rewards[(state, action, new_state)] = reward\n",
    "        transitions[(state, action)][new_state] += 1\n",
    "        total_reward += reward\n",
    "        if is_done:\n",
    "            break\n",
    "        state = new_state\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5  Reward:  0.15\n",
      "Epoch:  7  Reward:  0.35\n",
      "Epoch:  10  Reward:  0.45\n",
      "Epoch:  13  Reward:  0.5\n",
      "Epoch:  14  Reward:  0.65\n",
      "Epoch:  15  Reward:  0.75\n",
      "Epoch:  18  Reward:  0.8\n",
      "Solved in 19 epochs!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    while reward < SOLVED:\n",
    "\n",
    "        #Explore for 100 steps\n",
    "        explore(100)\n",
    "\n",
    "        #Value Iteration\n",
    "        update()\n",
    "\n",
    "        # Test X episodes\n",
    "        reward = test(TEST_EPISODES)\n",
    "\n",
    "        if reward > threshold:\n",
    "            print(\"Epoch: \",epoch,\" Reward: \", reward)\n",
    "            threshold = reward\n",
    "\n",
    "        epoch += 1\n",
    "        \n",
    "    print(\"Solved in %d epochs!\" % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
